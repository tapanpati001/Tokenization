{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169a0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'common', 'task', 'in', 'Natural', 'Language', 'Processing', '(NLP).', 'Itâ€™s', 'a', 'fundamental', 'step', 'in', 'both', 'traditional', 'NLP', 'methods', 'like', 'Count', 'Vectorizer', 'and', 'Advanced', 'Deep', 'Learning-based', 'architectures', 'like', 'Transformers.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Tokenization is a common task in Natural Language Processing (NLP). Itâ€™s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\"\"\"\n",
    "tokens=text.split()\n",
    "print(tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d57a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " ' But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be fool proof with split() method',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be fool proof with split() method.\"\"\"\n",
    "text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c280f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', '034', 'common', 'task', 'in', 'Natural', 'Language', 'Processing', 'NLP', 'It', 's', 'a', 'fundamental', 'step', 'in', 'both', 'traditional', 'NLP', 'methods', 'like', 'Count', 'Vectorizer', 'and', 'Advanced', 'Deep', 'Learning', 'based', 'architectures', 'like', 'Transformers']\n"
     ]
    }
   ],
   "source": [
    "#using regular expression \n",
    "import re \n",
    "text = \"\"\"Tokenization is a 034 common task in @ # Natural Language Processing (NLP). Itâ€™s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\"\"\"\n",
    "\n",
    "tokens=re.findall(\"[\\w]+\",text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0f441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " ' But one drawback with split() method, that we can only use one separator at a time',\n",
       " ' So sentence tonenization wont be foolproof with split() method',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "sent=re.compile('[.!?]').split(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3b111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tpati\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4484d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tpati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks', '.', 'Tokenization', 'breaks', 'the', 'raw', 'text', 'into', 'words', ',', 'sentences', 'called', 'tokens', '.', 'These', 'tokens', 'help', 'in', 'understanding', 'the', 'context', 'or', 'developing', 'the', 'model', 'for', 'the', 'NLP', '.', 'The', 'tokenization', 'helps', 'in', 'interpreting', 'the', 'meaning', 'of', 'the', 'text', 'by', 'analyzing', 'the', 'sequence', 'of', 'the', 'words', '.', '#', 'Hope', '#', 'shivan.K']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"\"\"Tokenization is breaking the raw text into small chunks. \n",
    "            Tokenization breaks the raw text into words, sentences called tokens. \n",
    "            These tokens help in understanding the context or developing the model \n",
    "            for the NLP. The tokenization helps in interpreting the meaning of \n",
    "            the text by analyzing the sequence of the words. #Hope #shivan.K\"\"\"\n",
    "\n",
    "tokens = word_tokenize(text1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7838b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks', '.', 'Tokenization', 'breaks', 'the', 'raw', 'text', 'into', 'words', ',', 'sentences', 'called', 'tokens', '.', 'These', 'tokens', 'help', 'in', 'understanding', 'the', 'context', 'or', 'developing', 'the', 'model', 'for', 'the', 'NLP', '.', 'The', 'tokenization', 'helps', 'in', 'interpreting', 'the', 'meaning', 'of', 'the', 'text', 'by', 'analyzing', 'the', 'sequence', 'of', 'the', 'words', '.', '#', 'Hope', '#', 'shivan.K']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645305a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is breaking the raw text into small chunks.',\n",
       " 'Tokenization breaks the raw text into words, sentences called tokens.',\n",
       " 'These tokens help in understanding the context or developing the model \\n            for the NLP.',\n",
       " 'The tokenization helps in interpreting the meaning of \\n            the text by analyzing the sequence of the words.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. \n",
    "            Tokenization breaks the raw text into words, sentences called tokens. \n",
    "            These tokens help in understanding the context or developing the model \n",
    "            for the NLP. The tokenization helps in interpreting the meaning of \n",
    "            the text by analyzing the sequence of the words.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c9044e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't take cryptocurrency advice from people on TwitterðŸ˜…ðŸ¤­\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Don't take cryptocurrency advice from people on TwitterðŸ˜…ðŸ¤­\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25f53849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'ðŸ˜…', 'ðŸ¤­']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer \n",
    "text = \"Don't take cryptocurrency advice from people on TwitterðŸ˜…ðŸ¤­\"\n",
    "tokenizer=TweetTokenizer()\n",
    "print(tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcc0bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'take',\n",
       " 'cryptocurrency',\n",
       " 'advice',\n",
       " 'from',\n",
       " 'people',\n",
       " 'on',\n",
       " 'TwitterðŸ˜…ðŸ¤­']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = \"Don't take cryptocurrency advice from people on TwitterðŸ˜…ðŸ¤­\"\n",
    "\n",
    "\n",
    "word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5de1095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.1-cp311-cp311-win_amd64.whl (12.0 MB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.12-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp311-cp311-win_amd64.whl (18 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp311-cp311-win_amd64.whl (479 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Collecting pydantic-core==2.6.3\n",
      "  Downloading pydantic_core-2.6.3-cp311-none-win_amd64.whl (1.7 MB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp311-cp311-win_amd64.whl (7.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: pydantic-core, catalogue, annotated-types, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 confection-0.1.1 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-2.3.0 pydantic-core-2.6.3 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.12 typer-0.9.0 wasabi-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tpati\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5697cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.5.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.6.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tpati\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acfbb787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks', '.', 'Tokenization', 'breaks', 'the', 'raw', 'text', 'into', 'words', ',', 'sentences', 'called', 'tokens', '.', 'These', 'tokens', 'help', 'in', 'understanding', 'the', 'context', 'or', 'developing', 'the', 'model', 'for', 'the', 'NLP', '.', 'The', 'tokenization', 'helps', 'in', 'interpreting', 'the', 'meaning', 'of', 'the', 'text', 'by', 'analyzing', 'the', 'sequence', 'of', 'the', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English \n",
    "nlp=English()\n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\"\"\"\n",
    "my_doc=nlp(text)\n",
    "\n",
    "token_list=[]\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de00a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.28.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.6.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tpati\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tpati\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb4aa175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'learning', 'NLP', 'using', 'spaCy']\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "doc=nlp(\"We are learning NLP using spaCy\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ddd97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks', '.', 'Tokenization', 'breaks', 'the', 'raw', 'text', 'into', 'words', ',', 'sentences', 'called', 'tokens', '.', 'These', 'tokens', 'help', 'in', 'understanding', 'the', 'context', 'or', 'developing', 'the', 'model', 'for', 'the', 'NLP', '.', 'The', 'tokenization', 'helps', 'in', 'interpreting', 'the', 'meaning', 'of', 'the', 'text', 'by', 'analyzing', 'the', 'sequence', 'of', 'the', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc060e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks', '.', 'Tokenization', 'breaks', 'the', 'raw', 'text', 'into', 'words', ',', 'sentences', 'called', 'tokens', '.', 'These', 'tokens', 'help', 'in', 'understanding', 'the', 'context', 'or', 'developing', 'the', 'model', 'for', 'the', 'NLP', '.', 'The', 'tokenization', 'helps', 'in', 'interpreting', 'the', 'meaning', 'of', 'the', 'text', 'by', 'analyzing', 'the', 'sequence', 'of', 'the', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb2bab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is breaking the raw text into small chunks.\n",
      "Tokenization breaks the raw text into words, sentences called tokens.\n",
      "These tokens help in understanding the context or developing the model for the NLP.\n",
      "The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a26e547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is breaking the raw text into small chunks.', 'Tokenization breaks the raw text into words, sentences called tokens.', 'These tokens help in understanding the context or developing the model for the NLP.', 'The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.']\n"
     ]
    }
   ],
   "source": [
    "nlp=English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "text = \"\"\"Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\"\"\"\n",
    "doc=nlp(text)\n",
    "sentence_list=[]\n",
    "for sentence in doc.sents:\n",
    "    sentence_list.append(sentence.text)\n",
    "    \n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc6492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
